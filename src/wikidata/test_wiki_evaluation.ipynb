{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pre-saved file /home/xzhao/workspace/probing-mulitlingual/result/dataset-mbert/mlama-dataset.pkl for MaskedDataset instance\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np \n",
    "sys.path.append('../../src/')\n",
    "\n",
    "from mask_dataset import MaskedDataset\n",
    "dataset = MaskedDataset(model_name=\"mbert\", reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_stacted_bar_and_percentage_of_fk_matching() got an unexpected keyword argument 'article'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwiki_evaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_stacted_bar_and_percentage_of_fk_matching\n\u001b[0;32m----> 2\u001b[0m plot_stacted_bar_and_percentage_of_fk_matching(dataset, article\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtokenized_article\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: plot_stacted_bar_and_percentage_of_fk_matching() got an unexpected keyword argument 'article'"
     ]
    }
   ],
   "source": [
    "from wiki_evaluation import plot_stacted_bar_and_percentage_of_fk_matching\n",
    "plot_stacted_bar_and_percentage_of_fk_matching(dataset, resource_type='tokenized_article')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import get_all_and_matched_uuid_lsts\n",
    "from collections import defaultdict\n",
    "import re, os, pickle\n",
    "from tqdm import tqdm \n",
    "\n",
    "root = \"/home/xzhao/workspace/probing-mulitlingual/datasets/2018_dump_wiki_cache/sub_obj_ggrep\"\n",
    "dump_root = \"/disk/xzhao/datasets/wikipedia_2018_octnov/sub_obj_ggrep_processed_result_dump\"\n",
    "os.makedirs(dump_root, exist_ok=True)\n",
    "def return_allthings(lang, tokenized2sub_uri, tokenized2obj_uri, rel2all_uuid, rel2matched_uuid):\n",
    "    dump_fn = os.path.join(dump_root, f\"{lang}.pkl\")\n",
    "    if os.path.exists(dump_fn):\n",
    "        with open(dump_fn, 'rb') as fp:\n",
    "            return pickle.load(fp)\n",
    "\n",
    "    lang_root = os.path.join(root, lang)\n",
    "    suburi2lineid = defaultdict(set)\n",
    "    objuri2lineid = defaultdict(set)\n",
    "\n",
    "    for subdir in os.listdir(lang_root):\n",
    "        subdir = os.path.join(lang_root, subdir)\n",
    "        for idx in range(10):\n",
    "            obj_match_fn = os.path.join(subdir, f\"object_match.txt.0{idx}\")\n",
    "            sub_match_fn = os.path.join(subdir, f\"subject_match.txt.0{idx}\")\n",
    "    \n",
    "            \n",
    "            file_id = subdir.split(\"/\")[-1] + f\":{idx}\"\n",
    "            with open(sub_match_fn, 'r') as fp:\n",
    "                for line in fp:\n",
    "                    line_id, *words = re.match(r'^(\\d+):(.*)$', line.strip()).groups()\n",
    "                    for word in map(str.strip, words[0].split('\\t')):\n",
    "                        for sub_uri in tokenized2sub_uri[word]:\n",
    "                            suburi2lineid[sub_uri].add(f\"{file_id}:{line_id}\")\n",
    "            with open(obj_match_fn, 'r') as fp:\n",
    "                for line in fp:\n",
    "                    line_id, *words = re.match(r'^(\\d+):(.*)$', line.strip()).groups()\n",
    "                    for word in map(str.strip, words[0].split('\\t')):\n",
    "                        for obj_uri in tokenized2obj_uri[word]:\n",
    "                            objuri2lineid[obj_uri].add(f\"{file_id}:{line_id}\")\n",
    "\n",
    "    # Read sub\n",
    "    uuid_info = dataset.get_uuid_info()\n",
    "    all_sub_obj_pairs = []\n",
    "    match_sub_obj_pairs = []\n",
    "    for rel, uuids in rel2all_uuid.items():\n",
    "        all_sub_obj_pairs.extend([(uuid_info[rel][uuid]['sub_uri'], uuid_info[rel][uuid]['obj_uri']) for uuid in uuids])        \n",
    "    for rel, uuids in rel2matched_uuid.items():\n",
    "        match_sub_obj_pairs.extend([(uuid_info[rel][uuid]['sub_uri'], uuid_info[rel][uuid]['obj_uri']) for uuid in uuids])\n",
    "    with open(dump_fn, 'wb') as fp:\n",
    "        pickle.dump((suburi2lineid, objuri2lineid, all_sub_obj_pairs, match_sub_obj_pairs), fp)\n",
    "    return suburi2lineid, objuri2lineid, all_sub_obj_pairs, match_sub_obj_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating resource for measuring factual knowledge existence in wiki and matches by ML-LMs: 100%|██████████| 53/53 [00:05<00:00,  9.57it/s]\n",
      "0it [00:14, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/xzhao/softwares/anaconda3/envs/probing/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/tmp/ipykernel_657831/2883268827.py\", line 48, in return_allthings\n    with open(dump_fn, 'wb') as fp:\n  File \"/home/xzhao/softwares/anaconda3/envs/probing/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 284, in _modified_open\n    return io_open(file, *args, **kwargs)\nFileNotFoundError: [Errno 2] No such file or directory: '/disk/xzhao/datasets/wikipedia_2018_octnov/sub_obj_ggrep_processed_result_dump/ga.pkl'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m ft \u001b[39min\u001b[39;00m tqdm(as_completed(futures)):\n\u001b[0;32m---> 22\u001b[0m     suburi2lineid, objuri2lineid, all_sub_obj_pairs, match_sub_obj_pairs \u001b[39m=\u001b[39m ft\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m     23\u001b[0m     \u001b[39m# lang2matching_measurement[lang] = (suburi2lineid, objuri2lineid, all_sub_obj_pairs, match_sub_obj_pairs)\u001b[39;00m\n",
      "File \u001b[0;32m~/softwares/anaconda3/envs/probing/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n",
      "File \u001b[0;32m~/softwares/anaconda3/envs/probing/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/disk/xzhao/datasets/wikipedia_2018_octnov/sub_obj_ggrep_processed_result_dump/ga.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m         lang2tokenized2obj_uri[lang][\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(info[\u001b[39m'\u001b[39m\u001b[39mobj_tokens\u001b[39m\u001b[39m'\u001b[39m])]\u001b[39m.\u001b[39madd(obj_uri)\n\u001b[1;32m     13\u001b[0m lang2matching_measurement \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 14\u001b[0m \u001b[39mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m53\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m     15\u001b[0m     futures \u001b[39m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     \u001b[39mfor\u001b[39;00m lang \u001b[39min\u001b[39;00m tqdm(dataset\u001b[39m.\u001b[39mlangs, desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating resource for measuring factual knowledge existence in wiki and matches by ML-LMs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m         \u001b[39m# return_allthings(lang, lang2tokenized2sub_uri[lang], lang2tokenized2obj_uri[lang], lang2rel2all_uuid[lang], lang2rel2matched_uuid[lang])\u001b[39;00m\n",
      "File \u001b[0;32m~/softwares/anaconda3/envs/probing/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    650\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/softwares/anaconda3/envs/probing/lib/python3.10/concurrent/futures/process.py:780\u001b[0m, in \u001b[0;36mProcessPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_manager_thread_wakeup\u001b[39m.\u001b[39mwakeup()\n\u001b[1;32m    779\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_manager_thread \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m wait:\n\u001b[0;32m--> 780\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_executor_manager_thread\u001b[39m.\u001b[39;49mjoin()\n\u001b[1;32m    781\u001b[0m \u001b[39m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[39m# objects that use file descriptors.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor_manager_thread \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/softwares/anaconda3/envs/probing/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/softwares/anaconda3/envs/probing/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "lang2rel2all_uuid, lang2rel2matched_uuid = get_all_and_matched_uuid_lsts(dataset=dataset, reload=False)\n",
    "\n",
    "lang2tokenized2sub_uri = defaultdict(lambda: defaultdict(set))\n",
    "for sub_uri, lang2info in dataset.get_sub_info().items():\n",
    "    for lang, info in lang2info.items():\n",
    "        lang2tokenized2sub_uri[lang][' '.join(info['sub_tokens'])].add(sub_uri)\n",
    "lang2tokenized2obj_uri = defaultdict(lambda: defaultdict(set))\n",
    "for obj_uri, lang2info in dataset.get_obj_info().items():\n",
    "    for lang, info in lang2info.items():\n",
    "        lang2tokenized2obj_uri[lang][' '.join(info['obj_tokens'])].add(obj_uri)\n",
    "\n",
    "lang2matching_measurement = {}\n",
    "with ProcessPoolExecutor(max_workers=53) as executor:\n",
    "    futures = []\n",
    "    for lang in tqdm(dataset.langs, desc=\"Generating resource for measuring factual knowledge existence in wiki and matches by ML-LMs\"):\n",
    "        # return_allthings(lang, lang2tokenized2sub_uri[lang], lang2tokenized2obj_uri[lang], lang2rel2all_uuid[lang], lang2rel2matched_uuid[lang])\n",
    "        dump_fn = os.path.join(dump_root, f\"{lang}.pkl\")\n",
    "        futures.append(executor.submit(return_allthings, lang, lang2tokenized2sub_uri[lang], lang2tokenized2obj_uri[lang], lang2rel2all_uuid[lang], lang2rel2matched_uuid[lang]))\n",
    "        # suburi2lineid, objuri2lineid, all_sub_obj_pairs, match_sub_obj_pairs = return_allthings(lang, lang2tokenized2sub_uri[lang], lang2tokenized2obj_uri[lang], lang2rel2all_uuid[lang], lang2rel2matched_uuid[lang])    \n",
    "    for ft in tqdm(as_completed(futures)):\n",
    "        suburi2lineid, objuri2lineid, all_sub_obj_pairs, match_sub_obj_pairs = ft.result()\n",
    "        # lang2matching_measurement[lang] = (suburi2lineid, objuri2lineid, all_sub_obj_pairs, match_sub_obj_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pred_evaluation import calculate_overall_p1_score_standard, get_full_match_matrix, calculate_overall_p1_score_from_match_matrix\n",
    "import numpy as np \n",
    "langs, p1 = calculate_overall_p1_score_standard(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pred_evaluation import get_all_and_matched_uuid_lsts\n",
    "from wiki_evaluation import _get_subject_object_cooccurence_in_article\n",
    "\n",
    "def display_matches_and_wiki_info(dataset, tgt_lang, display_content=False):\n",
    "    uuid2wikimatches = _get_subject_object_cooccurence_in_article(dataset, dataset.langs)[tgt_lang]\n",
    "    rel2predicted_uuid = get_all_and_matched_uuid_lsts(dataset=dataset, reload=False)[1][tgt_lang]\n",
    "    predicted_uuids = []\n",
    "    for rel in rel2predicted_uuid:\n",
    "        predicted_uuids.extend(rel2predicted_uuid[rel])\n",
    "    \n",
    "    nonwiki_uuids = [uuid for uuid in predicted_uuids if uuid2wikimatches[uuid] == 0]\n",
    "    print(f\"Matched factual knowledge in {tgt_lang}: {len(predicted_uuids)}\")\n",
    "    print(f\"Matched factual knowledge & No occurrence in wiki in {tgt_lang}: {len(nonwiki_uuids)}\")\n",
    "    print(f\"Non-wiki & Predicted Rate: {len(nonwiki_uuids)/len(predicted_uuids)}\")\n",
    "    \n",
    "    for rel in rel2predicted_uuid:\n",
    "        if len(rel2predicted_uuid[rel]) > 0:\n",
    "            nonwiki_uuids_by_rel = [uuid for uuid in rel2predicted_uuid[rel] if uuid in nonwiki_uuids]\n",
    "            print(f\"{dataset.display_rel(rel)}\\nall-matches: {len(rel2predicted_uuid[rel])}, nonwiki-matches: {len(nonwiki_uuids_by_rel)}\")\n",
    "            if nonwiki_uuids_by_rel:\n",
    "                print(f\"{dataset.display_uuid(nonwiki_uuids_by_rel[-1])}\")\n",
    "                print(f\"{dataset.display_uuid(nonwiki_uuids_by_rel[-1], lang=tgt_lang)}\\n\")\n",
    "            else:\n",
    "                print(f\"{dataset.display_uuid(rel2predicted_uuid[rel][-1])}\")\n",
    "                print(f\"{dataset.display_uuid(rel2predicted_uuid[rel][-1], lang=tgt_lang)}\\n\")\n",
    "\n",
    "display_matches_and_wiki_info(dataset, \"af\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wiki_evaluation import get_wiki_matches_matrix_from_dumped_wiki_article, _get_subject_object_cooccurence_in_article\n",
    "# lang2uuid2matches = _get_subject_object_cooccurence_in_article(dataset, candidate_langs=dataset.langs, reload=True)\n",
    "# langs, all_uuids, sub_matrix = get_wiki_matches_matrix_from_dumped_wiki_article(dataset, reload=True)\n",
    "\n",
    "from wiki_evaluation import plot_stacted_bar_and_percentage_of_fk_matching\n",
    "plot_stacted_bar_and_percentage_of_fk_matching(dataset, resource_type=\"article\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(sub_matrix[0]==2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uuid in lang2uuid2matches['zh']:\n",
    "    if lang2uuid2matches['zh'][uuid]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonwiki_uuid_idxs = np.where(sub_matrix[langs.index('zh')]==2)[0]\n",
    "uuid_infos = dataset.get_uuid_info_all_lang()\n",
    "for idx in nonwiki_uuid_idxs[:30]:\n",
    "    uuid_info = uuid_infos[all_uuids[idx]]['zh']\n",
    "    print(uuid_info['sub'], uuid_info['obj'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/xzhao/workspace/probing-mulitlingual/result/evaluation-mbert/wiki_and_probing_matching_matrix_from_dumped_wiki_article.pkl\"\n",
    "import pickle \n",
    "with open(data_path, 'rb') as fp:\n",
    "    langs, all_uuids, sub_matrix = pickle.load(fp)\n",
    "\n",
    "# sub_matrix[langs.index('en')].shape\n",
    "len(np.where(sub_matrix[langs.index('zh')]==2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from wiki_evaluation import get_uri2file2lineids_from_article_grep_matching\n",
    "\n",
    "TGT_DATA_ROOT = \"/disk/xzhao/probing-multilingual/2018_dump_wiki_cache\"\n",
    "lang = 'en'\n",
    "sub_grep_matching_root = os.path.join(TGT_DATA_ROOT, \"article_grep_matched\", 'sub', lang)\n",
    "obj_grep_matching_root = os.path.join(TGT_DATA_ROOT, \"article_grep_matched\", 'obj', lang)\n",
    "\n",
    "sub2file2ids = get_uri2file2lineids_from_article_grep_matching(sub_grep_matching_root, 'subject')\n",
    "obj2file2ids = get_uri2file2lineids_from_article_grep_matching(obj_grep_matching_root, 'object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uuid in np.where(sub_matrix[0]==2)[0][:10]:\n",
    "    print(all_uuids[uuid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import _get_subject_object_cooccurence_in_article, get_all_and_matched_uuid_lsts, measure_crosslingual_transfer_by_correlation_corrtpred_inwiki_fk\n",
    "sorted_langs, lang2rate = measure_crosslingual_transfer_by_correlation_corrtpred_inwiki_fk(dataset, per_lang=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_single(x_langs, line_langs, lang2rate):\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    x = np.array(list(range(len(x_langs))))\n",
    "    for lang in line_langs:\n",
    "        ax.plot(x, np.array(lang2rate[lang]))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_langs, fontsize=8, rotation=90)\n",
    "    plt.title(\"P1 scores for different relations per language\", fontsize=12)\n",
    "    ax.legend([dataset.display_lang(lang) for lang in x_langs], ncol=1, bbox_to_anchor=(1.0, 0.5), loc='center left')\n",
    "    plt.show()\n",
    "\n",
    "def draw_heatmap_for_pairwise_langsim_based_on_uuid_matching_rate(langs, lang2rate):\n",
    "    transfer_matrix = np.zeros((len(langs), len(langs)))\n",
    "    for base_lang in langs:\n",
    "        for test_lang in langs:\n",
    "            transfer_matrix[langs.index(base_lang)][langs.index(test_lang)] = lang2rate[test_lang][langs.index(base_lang)]\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    fig, ax = plt.subplots(figsize=(12,8))\n",
    "    langs_name = [dataset.display_lang(lang) for lang in langs]\n",
    "    g = sns.heatmap(transfer_matrix, xticklabels=langs_name, yticklabels=langs_name, cmap=\"GnBu\") # type: ignore\n",
    "    g.set_facecolor('#902008')\n",
    "    plt.title(\"Possiblity of transfer knowledge from language in Y-axis to X-axis. \\nThe languages are ranked by p1 scores\", fontsize=12)\n",
    "\n",
    "# draw_single(sorted_langs, sorted_langs[:10], lang2rate)\n",
    "# draw_single(sorted_langs, ['af'], lang2rate)\n",
    "draw_heatmap_for_pairwise_langsim_based_on_uuid_matching_rate(sorted_langs, lang2rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import plot_stacked_bar_and_percentage_of_inwiki_prediction\n",
    "plot_stacked_bar_and_percentage_of_inwiki_prediction(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import _get_title_object_subject_matchings, plot_stacted_bar_and_percentage_of_fk_matching, get_wiki_matches_matrix_from_dumped_wiki_title\n",
    "# aa = _get_title_object_subject_matchings(dataset=dataset, reload=True)\n",
    "# get_wiki_matches_matrix_from_dumped_wiki_title(dataset=dataset, reload=True)\n",
    "plot_stacted_bar_and_percentage_of_fk_matching(dataset, resource_type=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import get_wiki_matches_matrix_from_dumped_wiki_article, plot_stacted_bar_and_percentage_of_fk_matching, _get_subject_object_cooccurence_in_article\n",
    "from wiki_2018_dump import _preprocess_uri2file2lineids_from_article_grep_matching\n",
    "from wiki_evaluation import get_all_and_matched_uuid_lsts\n",
    "# aa = _get_title_object_subject_matchings(dataset=dataset, reload=True)\n",
    "# get_wiki_matches_matrix_from_dumped_wiki_title(dataset=dataset, reload=True)\n",
    "\n",
    "# candidate_langs=['ca', 'da', 'fi', 'ga', 'he', 'ja', 'ka', 'ko', 'ms', 'nl', 'ru', 'sr', 'th', 'zh', 'it']\n",
    "candidate_langs = dataset.langs\n",
    "# _preprocess_uri2file2lineids_from_article_grep_matching(candidate_langs)\n",
    "lang2uuid2wiki_matches = _get_subject_object_cooccurence_in_article(dataset, candidate_langs)\n",
    "lang2rel2matched_uuid = get_all_and_matched_uuid_lsts(dataset=dataset, reload=False)[1]\n",
    "plot_stacted_bar_and_percentage_of_fk_matching(dataset, resource_type=\"article\", candidate_langs=candidate_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import get_wiki_matches_matrix_from_dumped_wiki_article\n",
    "langs, all_uuids, sub_matrix = get_wiki_matches_matrix_from_dumped_wiki_article(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import display_niwp_fk_by_article_matching\n",
    "display_niwp_fk_by_article_matching(dataset, 'de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
