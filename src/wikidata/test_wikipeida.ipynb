{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../../src/')\n",
    "\n",
    "from mask_dataset import MaskedDataset\n",
    "dataset = MaskedDataset(model_name=\"mbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_evaluation import evaluate_corr_between_wikipages_and_p1\n",
    "evaluate_corr_between_wikipages_p1(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for uuid in aaa['zh']:\n",
    "    if len(aaa['zh'][uuid]) > 0:\n",
    "        cnt += 1\n",
    "\n",
    "cnt, len(aaa['zh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wiki_2018_dump import check_entity_existence_in_titles\n",
    "lang2obj, lang2objuri = dataset.get_lang2objs()\n",
    "lang2sub, lang2suburi = dataset.get_lang2subs()\n",
    "SRC_DATA_ROOT = \"/disk/xzhao/datasets/wikipedia_2018_octnov\"\n",
    "\n",
    "def check_entity_existence_in_titles(subjects, lang):\n",
    "    import numpy as np\n",
    "    lang_path = os.path.join(SRC_DATA_ROOT, \"title\", lang)\n",
    "    assert len(os.listdir(lang_path)) == 1\n",
    "\n",
    "    # obj_prefix_sets = set([obj[:3] for obj in objects])\n",
    "    sub_prefix_sets = set([sub[:3] for sub in subjects])\n",
    "\n",
    "    title_fn = os.path.join(lang_path, os.listdir(lang_path)[0])\n",
    "    titles = set()\n",
    "    with open(title_fn, 'r') as fp:\n",
    "        for idx, line in enumerate(fp):\n",
    "            line = line.strip()\n",
    "            if idx == 0 or line == \"\":\n",
    "                continue\n",
    "            try:\n",
    "                assert len(line.split(\"\\t\")) == 2\n",
    "                title = line.split()[1]\n",
    "                # if title[:3] in obj_prefix_sets or title[:3] in sub_prefix_sets:\n",
    "                if title[:3] in sub_prefix_sets:\n",
    "                    titles.add(title)\n",
    "            except Exception as e:\n",
    "                aa = line.split('\\t')\n",
    "                print(f\"Assertion error: {aa}\")\n",
    "                \n",
    "    # obj_exist_label = np.zeros((len(objects), ))\n",
    "    unmatched_subs = []\n",
    "    sub_exist_label = np.zeros((len(subjects), ))\n",
    "    for idx, sub in enumerate(subjects):\n",
    "        if sub in titles:\n",
    "            sub_exist_label[idx] = 1\n",
    "        else:\n",
    "            unmatched_subs.append(sub)\n",
    "\n",
    "    # for idx, obj in enumerate(obj_prefix_sets):\n",
    "    #     if obj in titles:\n",
    "    #         obj_exist_label[idx] = 1\n",
    "\n",
    "    # return lang, obj_exist_label, sub_exist_label\n",
    "    return lang, sub_exist_label, unmatched_subs\n",
    "\n",
    "lang = 'en'\n",
    "lang, sub_exist_label, unmatched_subs = check_entity_existence_in_titles(lang2sub[lang], lang)\n",
    "print(len(sub_exist_label), sum(sub_exist_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_list(input_list, chunk_size):\n",
    "    res = []\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        res.append(input_list[i:i+chunk_size])\n",
    "    return res\n",
    "\n",
    "sub_info = dataset.sub_info\n",
    "lang2subs = {}\n",
    "for lang in dataset.langs:\n",
    "    subs = [(sub_info[sub_uri][lang]['sub'], sub_uri) for sub_uri in sub_info.keys() if lang in sub_info[sub_uri]]\n",
    "    sub_splitted = chunk_list(subs, 500)\n",
    "    lang2subs[lang] = sub_splitted\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lang2subs['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "ARTICLE_ROOT = \"/disk/xzhao/datasets/wikipedia_2018_octnov/article\"\n",
    "ARTICLE_GREP_RES_ROOT = \"/disk/xzhao/datasets/wikipedia_2018_octnov/article-grep\"\n",
    "TGT_DATA_ROOT = \"/disk/xzhao/probing-multilingual/2018_dump_wiki_cache\"\n",
    "\n",
    "\n",
    "def run_grep_match(entities: list[(str, str)], lang, out_fn):\n",
    "    out_root = os.path.join(TGT_DATA_ROOT, \"article_grep_matched\", lang)\n",
    "    os.makedirs(out_root, exist_ok=True)\n",
    "    out_fn = os.path.join(out_root, out_fn)\n",
    "    lang_path = os.path.join(ARTICLE_ROOT, lang, '*')\n",
    "    with open(out_fn, 'w') as fp:\n",
    "        for entity, entity_uri in entities:\n",
    "            p = subprocess.run(f'grep -n \"{entity}\" {lang_path}', shell=True, capture_output=True, text=True)\n",
    "            lines = p.stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                newline = f\"{entity_uri}:{entity}:{line}\"\n",
    "                fp.write(f\"{newline}\\n\")\n",
    "\n",
    "def chunk_list(input_list, chunk_size):\n",
    "    res = []\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        res.append(input_list[i:i+chunk_size])\n",
    "    return res\n",
    "\n",
    "sub_info = dataset.sub_info\n",
    "lang2subs = {}\n",
    "for lang in dataset.langs:\n",
    "    subs = [(sub_info[sub_uri][lang]['sub'], sub_uri) for sub_uri in sub_info.keys() if lang in sub_info[sub_uri]]\n",
    "    sub_splitted = chunk_list(subs, 500)\n",
    "    for idx, sub_list in enumerate(sub_splitted):\n",
    "        run_grep_match([(\"蘇丹艾哈邁德清真寺\", \"Qtest\")], \"zh\", f\"{lang}-{idx}.txt\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for lang in os.listdir(\"/disk/xzhao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "prefix = \"jawiki-20181120\"\n",
    "pattern = f\"{prefix}-pages-articles[0-9]*.xml-.*.bz2\"\n",
    "text = \"jawiki-20181120-pages-articles1.xml-p1p106175.bz2\"\n",
    "\n",
    "re.match(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer, BertJapaneseTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")\n",
    "tokenizer.tokenize(\"日本\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mask_dataset import MaskedDataset\n",
    "from tqdm import tqdm \n",
    "\n",
    "dataset = MaskedDataset(model_name=\"mbert\")\n",
    "WIKI_DUMP_RESULT_ROOT = \"/home/xzhao/workspace/probing-mulitlingual/datasets/2018_dump_wiki_cache2/subject_object\"\n",
    "os.makedirs(WIKI_DUMP_RESULT_ROOT, exist_ok=True)\n",
    "sub_info = dataset.get_sub_info()\n",
    "obj_info = dataset.get_obj_info()\n",
    "\n",
    "for lang in tqdm(dataset.langs, desc=\"Writing tokenized subject and object to text file\"):\n",
    "    file_root = os.path.join(WIKI_DUMP_RESULT_ROOT, lang)\n",
    "    os.makedirs(file_root, exist_ok=True)\n",
    "\n",
    "    sub_fn = os.path.join(file_root, \"subject.text\")\n",
    "    obj_fn = os.path.join(file_root, \"object.text\")\n",
    "\n",
    "    with open(sub_fn, 'w') as fp:\n",
    "        for lang2sub in sub_info.values():\n",
    "            if lang in lang2sub:\n",
    "                fp.write(f\"{' '.join(lang2sub[lang]['sub_tokens'])}\\n\")\n",
    "    with open(obj_fn, 'w') as fp:\n",
    "        for lang2obj in obj_info.values():\n",
    "            if lang in lang2obj:\n",
    "                fp.write(f\"{' '.join(lang2obj[lang]['obj_tokens'])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "SUBOBJ_ROOT = \"/home/xzhao/workspace/probing-mulitlingual/datasets/2018_dump_wiki_cache/tokenized_subject_object\"\n",
    "ABS_ROOT = \"/home/xzhao/workspace/probing-mulitlingual/datasets/2018_dump_wiki_cache/abstracts_tokenized\"\n",
    "TITLE_ROOT = \"/home/xzhao/workspace/probing-mulitlingual/datasets/2018_dump_wiki_cache/titles_tokenized\"\n",
    "\n",
    "langs = ['ms', 'ca', 'ko', 'he', 'fi', 'ga', 'ka', 'en', 'th', 'nl', \n",
    "        'zh', 'ja', 'eu', 'da', 'pt', 'ru', 'fr', 'sr', 'et', 'sv', \n",
    "        'hy', 'cy', 'sq', 'it', 'hi', 'hr', 'es', 'hu', 'bg', 'ta', \n",
    "        'sl', 'bn', 'de', 'id', 'uk', 'be', 'ceb', 'el', 'fa', 'pl', \n",
    "        'az', 'ar', 'la', 'gl', 'lt', 'cs', 'sk', 'lv', 'tr', 'af', \n",
    "        'vi', 'ur', 'ro']\n",
    "\n",
    "for lang in langs:\n",
    "    lang_path = os.path.join(TITLE_ROOT, lang)\n",
    "    for fn in os.listdir(lang_path):\n",
    "        tgt_fh = open(os.path.join(lang_path, \"all_title.txt\"), 'w')\n",
    "        if \"title.txt\" in fn:\n",
    "            fp = os.path.join(lang_path, fn)\n",
    "            with open(fp, 'r') as fh:\n",
    "                for line in fh:\n",
    "                    tgt_fh.write(line.replace('\\t', ' ') + '\\n')\n",
    "        tgt_fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from wiki_2018_dump import xml_doc_iterator\n",
    "file_path = \"/disk/xzhao/datasets/wikipedia_2018_octnov/abstract/en/enwiki-20181120-abstract.xml\"\n",
    "iterator = xml_doc_iterator(file_path)\n",
    "\n",
    "for i, _ in tqdm(enumerate(iterator)):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_2018_dump import locate_urls, get_title_gz_urls, get_abstract_gz_urls\n",
    "from tqdm import tqdm \n",
    "\n",
    "title_paths = get_title_gz_urls(reload=False)\n",
    "lang2abstract = get_abstract_gz_urls(reload=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikidata.graelo_wiki import load_retraining_text_by_lang\n",
    "matched = 0\n",
    "unmatched = 0\n",
    "for lang in dataset.langs:\n",
    "    matched_sub, unmatched_sub, _, matched_obj, unmatched_obj, _ = load_retraining_text_by_lang(dataset=dataset, lang='zh')\n",
    "    matched += matched_sub\n",
    "    matched += matched_obj\n",
    "    unmatched += unmatched_sub\n",
    "    unmatched += unmatched_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wikipedia = load_dataset(\"graelo/wikipedia\", \"20230601.zh\", split='train', cache_dir=\"/disk/xzhao/datasets/huggingface\")\n",
    "\n",
    "def search_index(queries, lang, search_dataset=None):\n",
    "    if search_dataset is None:\n",
    "        search_dataset = load_dataset(\"graelo/wikipedia\", f\"20230601.{lang}\", split='train', cache_dir=\"/disk/xzhao/datasets/huggingface\")\n",
    "    \n",
    "    search_dataset.load_elasticsearch_index('title', es_client=es_client, es_index_name=f\"{lang}_title\")\n",
    "    return search_dataset.get_nearest_examples_batch(index_name=\"title\", queries=queries, k=20)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.load_elasticsearch_index('title', es_client=es_client, es_index_name=\"zh_title\")\n",
    "_, retrieved_in_title = wikipedia.get_nearest_examples(\"title\", query, k=10)\n",
    "retrieved_in_title['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia.load_elasticsearch_index('text', es_client=es_client, es_index_name=\"zh_text\")\n",
    "_, retrieved_in_text = wikipedia.get_nearest_examples(\"text\", query, k=10)\n",
    "retrieved_in_text['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "def add_index(es_client, lang):\n",
    "    wikipedia = load_dataset(\"graelo/wikipedia\", f\"20230601.{lang}\", split='train', cache_dir=\"/disk/xzhao/datasets/huggingface\")\n",
    "    if not es_client.indices.exists(index=f\"{lang}_title\"):\n",
    "        print(f\"Start to index {lang} title\")\n",
    "        wikipedia.add_elasticsearch_index(column=\"title\", es_client=es_client, es_index_name=f\"{lang}_title\")\n",
    "    else:\n",
    "        print(f\"Indexing of {lang} title is already done\")\n",
    "    if not es_client.indices.exists(index=f\"{lang}_text\"):\n",
    "        print(f\"Start to index {lang} text\")\n",
    "        wikipedia.add_elasticsearch_index(column=\"text\", es_client=es_client, es_index_name=f\"{lang}_text\")\n",
    "    else:\n",
    "        print(f\"Indexing of {lang} text is already done\")\n",
    "    \n",
    "def search_index(query, lang):\n",
    "    wikipedia = load_dataset(\"graelo/wikipedia\", f\"20230601.{lang}\", split='train')   \n",
    "    wikipedia.load_elasticsearch_index('title', es_client=es_client, es_index_name=f\"{lang}_title\")\n",
    "    wikipedia.load_elasticsearch_index('title', es_client=es_client, es_index_name=f\"{lang}_text\")\n",
    "    _, retrieved_in_title = wikipedia.get_nearest_examples(\"title\", query, k=10)\n",
    "    _, retrieved_in_text = wikipedia.get_nearest_examples(\"title\", query, k=10)\n",
    "    return retrieved_in_title, retrieved_in_text\n",
    "\n",
    "# search_index(\"ルイ・ジュール・トロシュ\", 'ja')\n",
    "add_index(es_client, 'zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ロジェ・ニミエ\"\n",
    "retrieved_examples = search_index(query, 'ja')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(retrieved_examples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
